{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"JNLR","text":""},{"location":"#jnlr-jax-based-non-linear-reconciliation-and-learning","title":"JNLR Jax-based non-linear reconciliation and learning","text":"<p>J-NLR is a Python library for non-linear reconciliation, learning, and geometric analysis on constraint manifolds. Built on JAX, it leverages automatic differentiation and GPU/TPU acceleration to efficiently project predicted values onto surfaces defined by implicit constraints \\(f(z) = 0\\).</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Non-linear Reconciliation: Multiple solvers (Augmented Lagrangian, curvature-aware Newton, vanilla projections) for projecting forecasts onto constraint manifolds</li> <li>SHOULD Analysis: Curvature-based methods to determine when reconciliation is beneficial\u2014verify if RMSE is guaranteed to reduce before applying corrections</li> <li>Manifold Sampling: Sample from explicit (graph) or implicit manifolds using volume-weighted sampling, Latin hypercube, or Langevin dynamics on the constraint surface</li> <li>Mesh Generation: Create triangulated meshes from explicit parameterizations for visualization and geodesic computation</li> <li>Geodesics: Compute geodesic distances and shortest paths on manifolds via exact MMP algorithm or fast graph-based approximations; includes probabilistic scores like pointcloud geodesic distance</li> <li>Visualization: Interactive 3D rendering of manifolds, projections, and geodesic paths with Plotly</li> <li>JAX-native: Fully JIT-compiled and vectorized (<code>vmap</code>) for high-performance batch processing</li> </ul>"},{"location":"#api-documentation","title":"API Documentation","text":"<p>Explore the full API reference:</p> <ul> <li>Reconcile - Non-linear reconciliation solvers</li> <li>Should - SHOULD analysis for curvature-based decision making</li> <li>Stats - Statistical utilities</li> <li>Curvature Utils - Curvature computation utilities</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install using <code>uv</code> package manager:</p> <pre><code>uv pip install -e .\n</code></pre> <p>Or with pip:</p> <pre><code>pip install jnlr\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If you use JNLR in academic work, please cite the associated paper:</p> <p>Lorenzo Nespoli, Anubhab Biswas, Roberto Rocchetta, and Vasco Medici. \"Nonlinear reconciliation: Error reduction theorems.\" Transactions on Machine Learning Research (TMLR), 2026. OpenReview: https://openreview.net/forum?id=dXRWuogm3J</p>"},{"location":"#bibtex","title":"BibTeX","text":"<pre><code>@article{nespoli2026nonlinear_reconciliation,\n  title   = {Nonlinear reconciliation: Error reduction theorems},\n  author  = {Nespoli, Lorenzo and Biswas, Anubhab and Rocchetta, Roberto and Medici, Vasco},\n  journal = {Transactions on Machine Learning Research},\n  year    = {2026},\n  url     = {https://openreview.net/forum?id=dXRWuogm3J},\n  note    = {Accepted by TMLR}\n}\n</code></pre>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work has been funded by the Swiss State Secretariat for Education, Research and Innovation (SERI) under the Swiss contribution to the Horizon Europe projects DR-RISE (Horizon Europe, Grant Agreement No. 101104154) and REEFLEX (Horizon Europe, Grant Agreement No. 101096192).</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the JNLR API documentation. Select a module from the navigation to explore its functions and classes.</p>"},{"location":"api/#modules","title":"Modules","text":"Module Description Reconcile Non-linear reconciliation solvers including Augmented Lagrangian and curvature-aware Newton methods Should SHOULD analysis for determining when reconciliation is beneficial based on curvature Stats Statistical utilities for error analysis and metrics Curvature Utils Low-level curvature computation utilities"},{"location":"api/#quick-example","title":"Quick Example","text":"<pre><code>import jax.numpy as jnp\nfrom jnlr import reconcile, should\n\n# Define your constraint function\ndef constraint(z):\n    return z[0] + z[1] + z[2] - 1.0\n\n# Your predictions\npredictions = jnp.array([0.4, 0.3, 0.4])\n\n# Reconcile to satisfy the constraint\nreconciled = reconcile.project(predictions, constraint)\n</code></pre>"},{"location":"api/curvature_utils/","title":"Curvature utils","text":""},{"location":"api/curvature_utils/#jnlr.utils.curvature_utils","title":"jnlr.utils.curvature_utils","text":""},{"location":"api/curvature_utils/#jnlr.utils.curvature_utils.tangent_space_basis","title":"tangent_space_basis","text":"<pre><code>tangent_space_basis(nu: ndarray)\n</code></pre> <p>Return an orthonormal basis for the tangent space orthogonal to \\(nu \u2208 R^n\\)</p> Source code in <code>src/jnlr/utils/curvature_utils.py</code> <pre><code>@jax.jit\ndef tangent_space_basis(nu: jnp.ndarray):\n    r\"\"\"Return an orthonormal basis for the tangent space orthogonal to $nu \u2208 R^n$\"\"\"\n    n = nu.shape[0]\n    eye = jnp.eye(n)\n    A = jnp.concatenate([nu[None, :], eye], axis=0)\n    Q, _ = jnp.linalg.qr(A.T)\n    return Q[:, 1:]  # Drop the component along nu\n</code></pre>"},{"location":"api/curvature_utils/#jnlr.utils.curvature_utils.solve_lagrange_multipliers","title":"solve_lagrange_multipliers","text":"<pre><code>solve_lagrange_multipliers(J: ndarray, delta_pi: ndarray, reg=1e-10)\n</code></pre> <p>Solve $$ J J^T \u03bb = -2 J \u03b4_\u03c0 $$ for \u03bb  (m\u00d7m system). A small Tikhonov term reg\u00b7I protects against rank\u2011deficiency.</p> <p>Parameters:</p> Name Type Description Default <code>J</code> <code>ndarray</code> <p>Jacobian matrix of shape (m, n)</p> required <code>delta_pi</code> <code>ndarray</code> <p>Difference vector of shape (n,)</p> required <code>reg</code> <p>Regularization parameter</p> <code>1e-10</code> <p>Returns:</p> Name Type Description <code>\u03bb</code> <p>Lagrange multipliers of shape (m,)</p> Source code in <code>src/jnlr/utils/curvature_utils.py</code> <pre><code>def solve_lagrange_multipliers(J: jnp.ndarray,\n                               delta_pi: jnp.ndarray,\n                               reg=1e-10):\n    r\"\"\"\n    Solve\n    $$\n    J J^T \u03bb = -2 J \u03b4_\u03c0\n    $$\n    for \u03bb  (m\u00d7m system).\n    A small Tikhonov term reg\u00b7I protects against rank\u2011deficiency.\n\n\n    Args:\n        J: Jacobian matrix of shape (m, n)\n        delta_pi: Difference vector of shape (n,)\n        reg: Regularization parameter\n\n    Returns:\n        \u03bb: Lagrange multipliers of shape (m,)\n\n    \"\"\"\n\n    JJt = J @ J.T\n    JJt_reg = JJt + reg * jnp.eye(JJt.shape[0], dtype=JJt.dtype)\n    rhs = -2.0 * (J @ delta_pi)       # shape (m,)\n    return jnp.linalg.solve(JJt_reg, rhs)   # \u03bb \u2208 R^m\n</code></pre>"},{"location":"api/curvature_utils/#jnlr.utils.curvature_utils.min_tangent_eigenvalue_vv","title":"min_tangent_eigenvalue_vv","text":"<pre><code>min_tangent_eigenvalue_vv(f, jacobian_f, hessian_f, z_tilde: ndarray, z_hat: ndarray, eps=1e-06) -&gt; jnp.ndarray\n</code></pre> <p>Generalization of min_tangent_eigenvalue to vector-valued constraint functions \\(F: R^n -&gt; R^m\\). Computes the minimum eigenvalue of the combined Hessian projected to the tangent space.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <p>Constraint function</p> required <code>jacobian_f</code> <p>Function returning (m, n) Jacobian matrix DF(z)</p> required <code>hessian_f</code> <p>Function returning (m, n, n) Hessians of each component function f_i</p> required <code>z_tilde</code> <code>ndarray</code> <p>Point at which to compute the curvature (projection of z_hat onto constraint surface)</p> required <code>z_hat</code> <code>ndarray</code> <p>Original point before projection</p> required <code>eps</code> <p>Small constant to avoid division by zero</p> <code>1e-06</code> <p>Returns:     A pair (min_eigenvalue, ratio) where:         - min_eigenvalue: Minimum eigenvalue of the combined Hessian projected to the tangent space         - ratio: Ratio used for assessing the curvature condition</p> Source code in <code>src/jnlr/utils/curvature_utils.py</code> <pre><code>@partial(jax.jit, static_argnames=('f', 'jacobian_f', 'hessian_f'))\ndef min_tangent_eigenvalue_vv(f, jacobian_f, hessian_f,  z_tilde: jnp.ndarray, z_hat: jnp.ndarray, eps=1e-6) -&gt; jnp.ndarray:\n    r\"\"\"\n    Generalization of min_tangent_eigenvalue to vector-valued constraint functions $F: R^n -&gt; R^m$.\n    Computes the minimum eigenvalue of the combined Hessian projected to the tangent space.\n\n    Args:\n        f: Constraint function\n        jacobian_f: Function returning (m, n) Jacobian matrix DF(z)\n        hessian_f: Function returning (m, n, n) Hessians of each component function f_i\n        z_tilde: Point at which to compute the curvature (projection of z_hat onto constraint surface)\n        z_hat: Original point before projection\n        eps: Small constant to avoid division by zero\n    Returns:\n        A pair (min_eigenvalue, ratio) where:\n            - min_eigenvalue: Minimum eigenvalue of the combined Hessian projected to the tangent space\n            - ratio: Ratio used for assessing the curvature condition\n    \"\"\"\n    delta_pi = z_tilde - z_hat\n    J = jacobian_f(z_tilde)       # (m, n)\n    grad_norms = jnp.linalg.norm(J, axis=1)\n    valid_mask = jnp.all(grad_norms &gt; 1e-6)\n\n    # ---------- original sufficient condition (safe) ----------\n    def compute_delta():\n        E = tangent_space_basis_vv(J)\n        Hs = hessian_f(z_tilde)\n        H_tan_sum = jnp.zeros((E.shape[1], E.shape[1]), dtype=z_tilde.dtype)\n        delta_proj = J @ delta_pi  # (m,)\n        kappa_max = 0.0\n        for i in range(Hs.shape[0]):\n            H_proj = E.T @ Hs[i] @ E\n            weight = -delta_proj[i] / (grad_norms[i] + 1e-8)\n            H_tan_sum += weight * H_proj\n            lam_abs = jnp.max(jnp.abs(jnp.linalg.eigvalsh(H_proj)))\n            kappa_i = lam_abs / (grad_norms[i] + eps)\n            kappa_max = jnp.maximum(kappa_max, kappa_i)\n\n        lam_min = jnp.linalg.eigvalsh(H_tan_sum)[0]\n\n        threshold = kappa_max * jnp.linalg.norm(delta_pi)\n        our_threshold = jnp.abs(lam_min) * jnp.linalg.norm(delta_pi)\n        return jnp.array([lam_min&gt;threshold, threshold])\n\n    # ---------- equivalent \u03bb\u2011based implementation ----------\n    def compute_lambda():\n        E = tangent_space_basis_vv(J)\n        Hs = hessian_f(z_tilde)\n\n        lam = solve_lagrange_multipliers(J, delta_pi)  # (m,)\n        mixed_vec = (J @ J.T) @ lam  # (m,)\n\n        H_tan_sum = jnp.zeros((E.shape[1], E.shape[1]), dtype=z_tilde.dtype)\n        for i in range(Hs.shape[0]):\n            H_proj = E.T @ Hs[i] @ E\n            weight = 0.5 * mixed_vec[i] / (grad_norms[i] + 1e-8)\n            H_tan_sum += weight * H_proj\n\n        eigvals = jnp.linalg.eigvalsh(H_tan_sum)\n        lam_max = jnp.max(-eigvals)\n        ratio = 1.0 / (jnp.abs(lam_max) + eps) / (jnp.linalg.norm(delta_pi) + eps)\n        return jnp.array([eigvals[0], ratio])\n\n    # choose either implementation; both are equivalent and safe\n    return jax.lax.cond(valid_mask, compute_delta, lambda: jnp.ones(2) * jnp.nan)\n</code></pre>"},{"location":"api/curvature_utils/#jnlr.utils.curvature_utils.min_eigenvalue","title":"min_eigenvalue","text":"<pre><code>min_eigenvalue(hessian_f, z: ndarray) -&gt; jnp.ndarray\n</code></pre> <p>Compute the minimum eigenvalue of the Hessian of \\(f\\) at \\(z\\).</p> <p>Parameters:</p> Name Type Description Default <code>hessian_f</code> <p>Function returning the Hessian matrix of shape (n, n)</p> required <code>z</code> <code>ndarray</code> <p>Point at which to compute the Hessian</p> required <p>Returns:     Minimum eigenvalue of the Hessian</p> Source code in <code>src/jnlr/utils/curvature_utils.py</code> <pre><code>@partial(jax.jit, static_argnames=('hessian_f'))\ndef min_eigenvalue(hessian_f, z: jnp.ndarray) -&gt; jnp.ndarray:\n    r\"\"\"\n    Compute the minimum eigenvalue of the Hessian of $f$ at $z$.\n\n\n    Args:\n        hessian_f: Function returning the Hessian matrix of shape (n, n)\n        z: Point at which to compute the Hessian\n    Returns:\n        Minimum eigenvalue of the Hessian\n\n    \"\"\"\n    H = hessian_f(z)\n    eigvals = jnp.linalg.eigvalsh(H)\n    return jnp.min(eigvals)\n</code></pre>"},{"location":"api/reconcile/","title":"Reconcile","text":""},{"location":"api/reconcile/#jnlr.reconcile","title":"jnlr.reconcile","text":""},{"location":"api/reconcile/#jnlr.reconcile.make_solver_alm_optax","title":"make_solver_alm_optax","text":"<pre><code>make_solver_alm_optax(f, w: ndarray = None, n_iterations: int = 30, tol_feas: float = 1e-08, rho0: float = 0.9, rho_mult: float = 10.0, rho_increase_thresh: float = 0.25, max_inner: int = 100, tol_grad: float = 1e-06, tol_step: float = 1e-10, lbfgs_learning_rate=None, lbfgs_memory_size: int = 10, ls_max_steps: int = 25, eps_chol: float = 1e-12, return_history: bool = False, vmapped: bool = True)\n</code></pre> <p>Returns: proj(zhat_batch) -&gt; z_proj_batch Projects onto \\({z : f(z)=0}\\) in metric W using ALM + Optax L-BFGS (zoom line search).</p> Source code in <code>src/jnlr/reconcile.py</code> <pre><code>def make_solver_alm_optax(\n    f,\n    w:jnp.ndarray = None,\n    # --- ALM (outer) ---\n    n_iterations: int = 30,\n    tol_feas: float = 1e-8,\n    rho0: float = 0.9,\n    rho_mult: float = 10.0,\n    rho_increase_thresh: float = 0.25,  # bump rho if ||f|| &gt; thresh*tol_feas\n\n    # --- LBFGS (inner) ---\n    max_inner: int = 100,\n    tol_grad: float = 1e-6,\n    tol_step: float = 1e-10,\n    lbfgs_learning_rate=None,  # let line search pick step by default\n    lbfgs_memory_size: int = 10,\n    ls_max_steps: int = 25,  # zoom line search budget\n\n    # numerics\n    eps_chol: float = 1e-12,\n    return_history: bool = False,\n    vmapped: bool = True,\n):\n    r\"\"\"\n    Returns: proj(zhat_batch) -&gt; z_proj_batch\n    Projects onto ${z : f(z)=0}$ in metric W using ALM + Optax L-BFGS (zoom line search).\n    \"\"\"\n\n    # --- Whitening W = L^T L ---\n    if w is None:\n        input_d, output_d = infer_io_shapes(f)\n        W = jnp.eye(input_d[0])\n    else:\n        W = jnp.asarray(w)\n\n    W = jnp.asarray(W)\n    W = 0.5 * (W + W.T)\n    n = W.shape[0]\n    L = jnp.linalg.cholesky(W + eps_chol * jnp.eye(n))\n    Linv = jnp.linalg.solve(L, jnp.eye(n))\n\n    # Build the (static) optimizer once; the objective is provided per-outer-iter\n    if lbfgs_learning_rate is None:\n        # legacy: use zoom (slower)\n        linesearch = optax.scale_by_zoom_linesearch(max_linesearch_steps=ls_max_steps)\n        solver = optax.lbfgs(\n            learning_rate=None,\n            memory_size=lbfgs_memory_size,\n            scale_init_precond=True,\n            linesearch=linesearch,\n        )\n    else:\n        # FAST path: fixed step, NO line search\n        solver = optax.lbfgs(\n            learning_rate=lbfgs_learning_rate,  # e.g., 1.0\n            memory_size=lbfgs_memory_size,\n            scale_init_precond=True,\n            linesearch=None,  # &lt;- critical\n        )\n    def feas_norm_y(y):\n        z = Linv @ y\n        return jnp.linalg.norm(jnp.atleast_1d(f(z)))\n\n    def inner_minimize(y_init, yhat, lam, rho):\n        r\"\"\"\n        Minimize $$L(y; lam, rho) = 0.5||y - yhat||^2 + lam^T c + 0.5*rho*||c||^2$$\n        with Optax L-BFGS + zoom LS.\n        \"\"\"\n        # Objective (captures yhat, lam, rho)\n        def L_value(y):\n            z = Linv @ y\n            c = jnp.atleast_1d(f(z))\n            d = y - yhat\n            return 0.5 * (d @ d) + jnp.dot(lam, c) + 0.5 * rho * (c @ c)\n\n        # Initialize optimizer state\n        y = y_init\n        opt_state = solver.init(y)\n\n        def cond_inner(state):\n            k, y, opt_state, step, gnorm = state\n            return (k &lt; max_inner) &amp; ((gnorm &gt; tol_grad) | (step &gt; tol_step))\n\n        def body_inner(state):\n            k, y, opt_state, _, _ = state\n            # Provide value &amp; grad explicitly; pass value_fn for the linesearch\n            value, grad = jax.value_and_grad(L_value)(y)\n            updates, opt_state = solver.update(\n                grad, opt_state, y, value=value, grad=grad, value_fn=L_value\n            )\n            y_new = optax.apply_updates(y, updates)\n            step = jnp.linalg.norm(optax.tree_utils.tree_norm(updates))\n            gnorm = jnp.linalg.norm(grad, ord=jnp.inf)\n            return (k + 1, y_new, opt_state, step, gnorm)\n\n        k0 = jnp.array(0)\n        step0 = jnp.array(jnp.inf)\n        gnorm0 = jnp.array(jnp.inf)\n        _, y_fin, _, _, _ = lax.while_loop(cond_inner, body_inner, (k0, y, opt_state, step0, gnorm0))\n        return y_fin\n\n\n    def solve_single(zhat):\n        yhat = L @ zhat\n        y = yhat\n        # initialize lambda without Python int casts (JAX-safe)\n        lam = jnp.zeros_like(jnp.atleast_1d(f(Linv @ y)))\n        rho = jnp.array(rho0)\n\n        if return_history:\n            # fixed-length scan; freeze updates after convergence\n            def body(carry, _):\n                k, y, lam, rho = carry\n                active = feas_norm_y(y) &gt; tol_feas\n\n                y_new = lax.cond(\n                    active,\n                    lambda _: inner_minimize(y, yhat, lam, rho),\n                    lambda _: y,\n                    operand=None,\n                )\n                z_new = Linv @ y_new\n                c_new = jnp.atleast_1d(f(z_new))\n                lam_prop = lam + rho * c_new\n                rho_prop = jnp.where(\n                    jnp.linalg.norm(c_new) &gt; rho_increase_thresh * tol_feas,\n                    rho * rho_mult,\n                    rho,\n                )\n\n                y_next = jnp.where(active, y_new, y)\n                lam_next = jnp.where(active, lam_prop, lam)\n                rho_next = jnp.where(active, rho_prop, rho)\n                return (k + 1, y_next, lam_next, rho_next), (Linv @ y_next)\n\n            (_, _, _, _), z_hist = lax.scan(body, (0, y, lam, rho), xs=None, length=n_iterations)\n            return z_hist  # (T, n)\n        else:\n            # original early-stop loop; return only the final iterate\n            def cond_outer(state):\n                k, y, lam, rho = state\n                return (k &lt; n_iterations) &amp; (feas_norm_y(y) &gt; tol_feas)\n\n            def body_outer(state):\n                k, y, lam, rho = state\n                y_new = inner_minimize(y, yhat, lam, rho)\n                z_new = Linv @ y_new\n                c_new = jnp.atleast_1d(f(z_new))\n                lam_new = lam + rho * c_new\n                rho_new = jnp.where(\n                    jnp.linalg.norm(c_new) &gt; rho_increase_thresh * tol_feas,\n                    rho * rho_mult,\n                    rho,\n                )\n                return (k + 1, y_new, lam_new, rho_new)\n\n            _, y_opt, _, _ = lax.while_loop(cond_outer, body_outer, (0, y, lam, rho))\n            return Linv @ y_opt\n\n\n\n    if vmapped:\n        return jax.jit(jax.vmap(solve_single))\n    else:\n        return jax.jit(solve_single)\n</code></pre>"},{"location":"api/reconcile/#jnlr.reconcile.make_solver_proj_nt_curv","title":"make_solver_proj_nt_curv","text":"<pre><code>make_solver_proj_nt_curv(f, w: ndarray = None, n_iterations: int = 30, n_tangent_micro: int = 3, alpha_n: float = 1.0, alpha_t: float = 0.7, beta: float = 0.5, max_bt: int = 6, damping: float = 1e-06, return_history: bool = False, vmapped: bool = True)\n</code></pre> <p>Constrained projection:</p> \\[\\text{arg}\\min_{z} \\tfrac{1}{2} (z - \\hat{z})^T W (z - \\hat{z})\\] \\[\\text{s.t. } f(z) = 0\\] Iteration <p>1) Normal correction at current z:</p> \\[s_n = -J^T (JJ^T + \\varepsilon I)^{-1} f(z);  z &lt;- z + \\alpha _n s_n\\] <p>2) Curvature-aware tangent slide (n_tangent_micro times):      relinearize at each sub-step:</p> \\[ Pt = I - J^T (JJ^T + \\varepsilon I)^{-1} J\\] \\[s_t = - Pt (z - \\hat z)\\] <p>accept \\(z + \\alpha s_t\\) only if \\(||f||\\) decreases; else \\alpha &lt;- \\beta\\alpha</p> <p>This keeps moves small, exploits curvature, and has very few knobs.</p> Source code in <code>src/jnlr/reconcile.py</code> <pre><code>def make_solver_proj_nt_curv(\n    f,\n    w:jnp.ndarray = None,\n    n_iterations: int = 30,        # outer iters\n    n_tangent_micro: int = 3,      # relinearized tangent micro-steps / iter\n    alpha_n: float = 1.0,          # normal step fraction\n    alpha_t: float = 0.7,          # initial tangent step fraction\n    beta: float = 0.5,             # shrink for tangent backtracking\n    max_bt: int = 6,               # max shrink attempts per micro-step\n    damping: float = 1e-6,         # tiny Tikhonov on JJ^T\n    return_history: bool = False,\n    vmapped: bool = True,\n):\n    r\"\"\"\n    Constrained projection:\n\n    $$\\text{arg}\\min_{z} \\tfrac{1}{2} (z - \\hat{z})^T W (z - \\hat{z})$$\n\n    $$\\text{s.t. } f(z) = 0$$\n\n    Iteration:\n      1) Normal correction at current z:\n\n      $$s_n = -J^T (JJ^T + \\varepsilon I)^{-1} f(z);  z &lt;- z + \\alpha _n s_n$$\n\n      2) Curvature-aware tangent slide (n_tangent_micro times):\n           relinearize at each sub-step:\n\n      $$ Pt = I - J^T (JJ^T + \\varepsilon I)^{-1} J$$\n\n      $$s_t = - Pt (z - \\hat z)$$\n\n      accept $z + \\alpha s_t$ only if $||f||$ decreases; else \\alpha &lt;- \\beta\\alpha\n    This keeps moves small, exploits curvature, and has very few knobs.\n    \"\"\"\n\n    jac_f = jax.jacfwd(f)\n\n    def f_norm(z):       # scalar feasibility\n        r = jnp.atleast_1d(f(z))\n        return jnp.sqrt(jnp.sum(r*r))\n\n    def normal_step(z):\n        J   = jnp.atleast_2d(jac_f(z))\n        m   = J.shape[0]\n        JJt = J @ J.T + damping * jnp.eye(m, dtype=J.dtype)\n        r   = jnp.atleast_1d(f(z))\n        lam = jnp.linalg.solve(JJt, r)\n        s_n = - J.T @ lam\n        return z + alpha_n * s_n\n\n    def one_tangent_micro(z, zhat, a):\n        # build tangent projector at *current* z (relinearize each micro step)\n        J   = jnp.atleast_2d(jac_f(z))\n        m   = J.shape[0]\n        JJt = J @ J.T + damping * jnp.eye(m, dtype=J.dtype)\n        Pt  = jnp.eye(J.shape[1], dtype=J.dtype) - J.T @ jnp.linalg.solve(JJt, J)\n        s_t = - Pt @ (z - zhat)         # restricted descent for 0.5||z-zhat||^2\n\n        phi0 = f_norm(z)\n\n        def bt_body(i, best):\n            z_best, accepted = best\n            a_i = beta**i * a\n            z_cand = z + a_i * s_t\n            phi_c  = f_norm(z_cand)\n            ok = phi_c &lt;= phi0   # accept only if feasibility doesn't worsen\n            z_new = jnp.where(accepted, z_best, jnp.where(ok, z_cand, z_best))\n            acc_new = jnp.logical_or(accepted, ok)\n            return (z_new, acc_new)\n\n        z_bt, acc = lax.fori_loop(0, max_bt, bt_body, (z, jnp.array(False)))\n        z_fb = z  # if nothing improved, skip tangent step this time\n        return jnp.where(acc, z_bt, z_fb)\n\n    def one_iteration(z, zhat):\n        # 1) normal correction\n        z = normal_step(z)\n        # 2) a few small tangent steps with relinearization + decrease guard on ||f||\n        def micro_body(_, state):\n            z_curr = state\n            return one_tangent_micro(z_curr, zhat, alpha_t)\n        z = lax.fori_loop(0, n_tangent_micro, micro_body, z)\n        return z\n\n    def scan_step(carry, _):\n        z, zhat = carry\n        z_next = one_iteration(z, zhat)\n        return (z_next, zhat), z_next\n\n    def solve_single(zhat: jnp.ndarray):\n        z0 = zhat\n        (zT, _), hist = lax.scan(scan_step, (z0, zhat), xs=None, length=n_iterations)\n        return hist if return_history else zT\n\n    if vmapped:\n        return jax.jit(jax.vmap(solve_single))\n    else:\n        return jax.jit(solve_single)\n</code></pre>"},{"location":"api/reconcile/#jnlr.reconcile.make_solver","title":"make_solver","text":"<pre><code>make_solver(f, w: ndarray = None, n_iterations: int = 50, damping: float = 1e-05, beta: float = 0.5, c_armijo: float = 0.0001, max_bt: int = 12, return_history: bool = False, vmapped: bool = True)\n</code></pre> <p>Create a v-mapped and JIT-compiled solver function for the constrained optimization problem. Here f is the implicit function representing the manifold constraints: \\(M = \\{ z : f(z) = 0 \\}\\). The returned function takes \\(\\hat z\\) as input and returns the projected z.</p> \\[\\text{arg}\\min_{z} \\tfrac{1}{2} (z - \\hat{z})^T W (z - \\hat{z})\\] \\[\\text{s.t. } f(z) = 0\\] <p>Parameters:</p> Name Type Description Default <code>f</code> <p>Function representing the constraints, in implicit form. The signature of f should be \\(f(z): \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) where \\(n\\) is the dimension of the input and m the output.</p> required <code>W</code> <p>Weight matrix.</p> required <code>n_iterations</code> <code>int</code> <p>Number of iterations for the learning process.</p> <code>50</code> <p>Returns:</p> Type Description <p>A JIT-compiled function that takes z_hat as input and returns the projected z.</p> Source code in <code>src/jnlr/reconcile.py</code> <pre><code>def make_solver(f,\n                w:jnp.ndarray = None,\n                n_iterations: int = 50,\n                damping: float = 1e-5,          # a bit larger for f32; reduce if using x64\n                beta: float = 0.5,              # backtracking factor\n                c_armijo: float = 1e-4,         # sufficient decrease on ||f||\n                max_bt: int = 12,\n                return_history: bool = False,\n                vmapped: bool = True):\n    r\"\"\"\n    Create a v-mapped and JIT-compiled solver function for the constrained optimization problem.\n    Here f is the implicit function representing the manifold constraints: $M = \\{ z : f(z) = 0 \\}$.\n    The returned function takes $\\hat z$ as input and returns the projected z.\n\n    $$\\text{arg}\\min_{z} \\tfrac{1}{2} (z - \\hat{z})^T W (z - \\hat{z})$$\n\n    $$\\text{s.t. } f(z) = 0$$\n\n    Args:\n        f: Function representing the constraints, in implicit form. The signature of f should be $f(z): \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ where $n$ is the dimension of the input and m the output.\n        W: Weight matrix.\n        n_iterations:  Number of iterations for the learning process.\n\n    Returns:\n        A JIT-compiled function that takes z_hat as input and returns the projected z.\n    \"\"\"\n\n    n_input, n_constraints = infer_io_shapes(f)\n\n    if w is None:\n        W = jnp.eye(n_input[0])\n    else:\n        W = jnp.asarray(w)\n    jac_f = jax.jacfwd(f)\n\n    #n_constraints = jnp.atleast_1d(f(jnp.zeros(W.shape[0]))).shape[0]\n\n    def step(z, lam, zhat):\n        J = jnp.atleast_2d(jac_f(z))  # (m,n)\n        g =  W @ (z - zhat) + J.T @ lam  # stationarity residual\n        cvec = jnp.atleast_1d(f(z))  # constraint residual (m,)\n\n        H = W + damping * jnp.eye(W.shape[0])  # (n,n)\n        Hinv_g = jnp.linalg.solve(H, g)\n        Hinv_Jt = jnp.linalg.solve(H, J.T)\n        S = J @ Hinv_Jt + damping * jnp.eye(J.shape[0])  # (m,m)\n\n        dlam = jnp.linalg.solve(S, cvec - J @ Hinv_g)  # (m,)\n        dz = -(Hinv_g + Hinv_Jt @ dlam)  # (n,)\n        return dz, dlam, cvec\n\n    def backtrack(z, dz, c0):\n        # reduce alpha until ||f(z+alpha dz)|| &lt;= (1 - c_armijo*alpha) ||f(z)||\n        alpha0 = 1.0\n        fnorm0 = jnp.linalg.norm(c0)\n\n        def body_fun(state):\n            k, alpha = state\n            zt = z + alpha * dz\n            fnorm = jnp.linalg.norm(jnp.atleast_1d(f(zt)))\n            ok = fnorm &lt;= (1.0 - c_armijo * alpha) * fnorm0\n            alpha = jnp.where(ok, alpha, beta * alpha)\n            return (k + 1, alpha)\n\n        def cond_fun(state):\n            k, alpha = state\n            zt = z + alpha * dz\n            fnorm = jnp.linalg.norm(jnp.atleast_1d(f(zt)))\n            return (k &lt; max_bt) &amp; (fnorm &gt; (1.0 - c_armijo * alpha) * fnorm0)\n\n        _, alpha = lax.while_loop(cond_fun, body_fun, (0, alpha0))\n        return alpha\n\n    if return_history:\n        def solve_single(zhat: jnp.ndarray) -&gt; jnp.ndarray:\n            z = zhat\n            lam = jnp.zeros(n_constraints)\n\n            def body(carry, _):\n                z, lam = carry\n                dz, dlam, cvec = step(z, lam, zhat)\n                alpha = backtrack(z, dz, cvec)\n                return (z + alpha * dz, lam + alpha * dlam), z + alpha * dz\n\n            (z_final, _), zs = lax.scan(body, (z, lam), xs=None, length=n_iterations)\n            return zs\n    else:\n        def solve_single(zhat: jnp.ndarray) -&gt; jnp.ndarray:\n            z = zhat\n            lam = jnp.zeros(n_constraints)\n\n            def body_fn(_, state):\n                z, lam = state\n                dz, dlam, cvec = step(z, lam, zhat)\n                alpha = backtrack(z, dz, cvec)\n                return (z + alpha * dz, lam + alpha * dlam)\n\n            z_final, _ = lax.fori_loop(0, n_iterations, body_fn, (z, lam))\n            return z_final\n\n    if vmapped:\n        return jax.jit(jax.vmap(solve_single))\n    else:\n        return jax.jit(solve_single)\n</code></pre>"},{"location":"api/should/","title":"Should","text":""},{"location":"api/should/#jnlr.should","title":"jnlr.should","text":""},{"location":"api/should/#jnlr.should.constant_sign_curvature","title":"constant_sign_curvature","text":"<pre><code>constant_sign_curvature(f, grad_f, hessian_f, vmapped_solver, z_hat: ndarray) -&gt; jnp.ndarray\n</code></pre> <p>Check if forecasting RMSE is guaranteed to reduce based on the curvature condition, for a hypersurface defined by f(z) = 0 having constant sign curvature.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <p>Constraint function.</p> required <code>grad_f</code> <p>Function returning the gradient vector of shape (n,).</p> required <code>hessian_f</code> <p>Function returning the Hessian matrix of shape (n, n).</p> required <code>vmapped_solver</code> <p>Function to project points onto the constraint surface.</p> required <code>z_hat</code> <code>ndarray</code> <p>Forecasted points of shape (batch_size, n).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Boolean array indicating if the curvature condition is satisfied for each point in z_hat</p> Notes <p>Theorem 1: For a hypersurface defined by f(z) = 0, forecasting RMSE is guaranteed to reduce if:</p> \\[\\lambda_{min}(H_{restricted}(\\tilde{z})) * f(\\hat{z}) &gt; 0\\] <p>where * \\( \\tilde{z} \\) is the projection of the forecasted point, \\(\\hat{z}\\), onto the surface defined by \\(f(z) = 0.\\)</p> Source code in <code>src/jnlr/should.py</code> <pre><code>@partial(jax.jit, static_argnames=('f', 'grad_f', 'hessian_f', 'vmapped_solver'))\ndef constant_sign_curvature(f, grad_f, hessian_f, vmapped_solver, z_hat: jnp.ndarray) -&gt; jnp.ndarray:\n    r\"\"\"\n    Check if forecasting RMSE is guaranteed to reduce based on the curvature condition, for a hypersurface defined by f(z) = 0 having constant sign curvature.\n\n    Args:\n        f: Constraint function.\n        grad_f: Function returning the gradient vector of shape (n,).\n        hessian_f: Function returning the Hessian matrix of shape (n, n).\n        vmapped_solver: Function to project points onto the constraint surface.\n        z_hat: Forecasted points of shape (batch_size, n).\n\n\n    Returns:\n        np.ndarray: Boolean array indicating if the curvature condition is satisfied for each point in z_hat\n\n\n    Notes:\n        Theorem 1: For a hypersurface defined by f(z) = 0, forecasting RMSE is guaranteed to reduce if:\n\n        $$\\lambda_{min}(H_{restricted}(\\tilde{z})) * f(\\hat{z}) &gt; 0$$\n\n        where\n        * \\( \\tilde{z} \\) is the projection of the forecasted point, $\\hat{z}$, onto the surface defined by $f(z) = 0.$\n    \"\"\"\n\n    z_tilde = vmapped_solver(z_hat)\n    lambda_min = jax.vmap(min_tangent_eigenvalue, in_axes=(None, None, 0))(grad_f,  hessian_f, z_tilde)\n    f_val = jax.vmap(f)(z_hat)\n    return lambda_min * f_val &gt; 0\n</code></pre>"},{"location":"api/should/#jnlr.should.vector_valued_convex","title":"vector_valued_convex","text":"<pre><code>vector_valued_convex(f, jacobian_F, hessians_F, vmapped_solver, z_hat: ndarray) -&gt; jnp.ndarray\n</code></pre> <p>Generalization of scalar curvature condition to vector-valued case.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <p>Constraint function.</p> required <code>jacobian_F</code> <p>Function returning \\((m, n)\\) Jacobian matrix \\(DF(z)\\).</p> required <code>hessians_F</code> <p>Function returning \\((m, n, n)\\) Hessians of each</p> required <code>vmapped_solver</code> <p>Function to project points onto the constraint surface.</p> required <code>z_hat</code> <code>ndarray</code> <p>Forecasted points of shape \\((batch_size, n)\\).</p> required <p>Returns:     np.ndarray: Boolean array indicating if the curvature condition is satisfied for each point in \\(\\hat{z}\\).</p> Notes <p>Check if lambda_min(H_combined) &gt;= 0, where: H_combined = \\(sum_i (\\delta_\\pi[i] / ||\u2207f_i||) * H_{tan_i}\\)</p> <p>where:     - \\(\\delta_\\pi = \\tilde{z} - \\hat{z}\\)     - \\(H_{tan_i} = E^T H_i E\\)  (Hessian projected to tangent space)</p> Source code in <code>src/jnlr/should.py</code> <pre><code>@partial(jax.jit, static_argnames=('f', 'jacobian_F', 'hessians_F', 'vmapped_solver'))\ndef vector_valued_convex(f, jacobian_F, hessians_F, vmapped_solver, z_hat: jnp.ndarray) -&gt; jnp.ndarray:\n    r\"\"\"\n    Generalization of scalar curvature condition to vector-valued case.\n\n    Args:\n        f: Constraint function.\n        jacobian_F: Function returning $(m, n)$ Jacobian matrix $DF(z)$.\n        hessians_F: Function returning $(m, n, n)$ Hessians of each\n        vmapped_solver: Function to project points onto the constraint surface.\n        z_hat: Forecasted points of shape $(batch_size, n)$.\n    Returns:\n        np.ndarray: Boolean array indicating if the curvature condition is satisfied for each point in $\\hat{z}$.\n\n    Notes:\n        Check if lambda_min(H_combined) &gt;= 0, where:\n        H_combined = $sum_i (\\delta_\\pi[i] / ||\u2207f_i||) * H_{tan_i}$\n    where:\n        - $\\delta_\\pi = \\tilde{z} - \\hat{z}$\n        - $H_{tan_i} = E^T H_i E$  (Hessian projected to tangent space)\n\n    \"\"\"\n\n    z_tilde = vmapped_solver(z_hat)\n    lambdas = jax.vmap(min_tangent_eigenvalue_vv, in_axes=(None, None, None, 0, 0))(f, jacobian_F,  hessians_F, z_tilde, z_hat)\n    lambda_min = lambdas[:, 0]  # Take the minimum eigenvalue from the pair (min, max)\n    ratios = lambdas[:, 1]\n    accepted = lambda_min &gt; 0\n    return accepted, ratios\n</code></pre>"},{"location":"api/should/#jnlr.should.p_reduction","title":"p_reduction","text":"<pre><code>p_reduction(vmapped_solver, z_hat: ndarray, z_hat_samples: ndarray, alpha=0.05)\n</code></pre> <p>Estimate the probability of RMSE reduction using projected bootstrap samples.</p> <p>Parameters:</p> Name Type Description Default <code>vmapped_solver</code> <p>Function to project points onto the constraint surface.</p> required <code>z_hat</code> <code>ndarray</code> <p>Forecasted points of shape <code>(batch_size, n)</code>.</p> required <code>z_hat_samples</code> <code>ndarray</code> <p>Bootstrap samples of shape <code>(batch_size, num_samples, n)</code>.</p> required <code>alpha</code> <p>Significance level for confidence intervals (not used by this function).</p> <code>0.05</code> <p>Returns:</p> Type Description <p>np.ndarray: Estimated probability of RMSE reduction for each point in <code>z_hat</code>.</p> Notes <p>Theorem 3. For a hypersurface defined by \\(f(z) = 0\\), the probability of forecasting RMSE reduction can be estimated as</p> \\[\\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}\\!\\left\\{\\,\\tilde{\\delta}_i^\\top \\delta_{\\pi} &gt; - \\frac{\\lVert \\delta_{\\pi} \\rVert^2}{2} \\right\\}.\\] <p>where</p> <ul> <li>\\( \\mathbf{1} \\) is the indicator function;</li> <li>\\( \\tilde{\\delta}_i = \\tilde{y}_i - \\tilde{y} \\);</li> <li>\\( \\tilde{y}_i \\) is the projection of the \\(i\\)-th sample onto the surface \\( f(z) = 0 \\);</li> <li>\\( \\tilde{y} \\) is the projection of \\( \\hat{y} \\) onto the surface \\( f(z) = 0 \\);</li> <li>\\( \\delta_{\\pi} = \\tilde{y} - \\hat{y} \\).</li> </ul> Source code in <code>src/jnlr/should.py</code> <pre><code>@partial(jax.jit, static_argnames=('vmapped_solver'))\ndef p_reduction(vmapped_solver, z_hat: jnp.ndarray, z_hat_samples: jnp.ndarray, alpha=0.05):\n    r\"\"\"\n    Estimate the probability of RMSE reduction using projected bootstrap samples.\n\n    Args:\n        vmapped_solver: Function to project points onto the constraint surface.\n        z_hat: Forecasted points of shape `(batch_size, n)`.\n        z_hat_samples: Bootstrap samples of shape `(batch_size, num_samples, n)`.\n        alpha: Significance level for confidence intervals (not used by this function).\n\n    Returns:\n        np.ndarray: Estimated probability of RMSE reduction for each point in `z_hat`.\n\n    Notes:\n        **Theorem 3.** For a hypersurface defined by $f(z) = 0$,\n        the probability of forecasting RMSE reduction can be estimated as\n\n        $$\\frac{1}{N} \\sum_{i=1}^{N}\n        \\mathbf{1}\\!\\left\\{\\,\\tilde{\\delta}_i^\\top \\delta_{\\pi}\n        &gt; - \\frac{\\lVert \\delta_{\\pi} \\rVert^2}{2} \\right\\}.$$\n\n\n        where\n\n        * \\( \\mathbf{1} \\) is the indicator function;\n        * \\( \\tilde{\\delta}_i = \\tilde{y}_i - \\tilde{y} \\);\n        * \\( \\tilde{y}_i \\) is the projection of the \\(i\\)-th sample onto the surface \\( f(z) = 0 \\);\n        * \\( \\tilde{y} \\) is the projection of \\( \\hat{y} \\) onto the surface \\( f(z) = 0 \\);\n        * \\( \\delta_{\\pi} = \\tilde{y} - \\hat{y} \\).\n    \"\"\"\n\n    z_tilde = vmapped_solver(z_hat)\n    z_tilde_samples = vmapped_solver(z_hat_samples.reshape(-1, z_hat.shape[-1])).reshape(z_hat_samples.shape)\n    delta_pi = z_tilde - z_hat\n\n    delta_tilde_samples = z_tilde_samples - z_tilde[:, None, :]\n    norm_delta_pi = jnp.linalg.norm(delta_pi, axis=-1, keepdims=True)\n    scalar_product = jnp.sum(delta_tilde_samples * delta_pi[:, None, :], axis=-1)\n    threshold = norm_delta_pi ** 2 / 2\n    condition = scalar_product &gt; - threshold\n\n    # cutoff\n    delta_tilde_hat_samples = z_tilde_samples - z_hat[:, None, :]\n    post_rec_dist = jnp.sum(delta_tilde_hat_samples**2, axis=-1)\n    bootstrap_errs = z_hat_samples - z_hat[:, None, :]\n    pre_rec_dist = jnp.sum(bootstrap_errs ** 2, axis=-1)\n    max_pre_rec_dist = jnp.max(pre_rec_dist, axis=-1, keepdims=True)\n    condition = jnp.where(post_rec_dist &lt; max_pre_rec_dist, condition, jnp.nan)\n\n    return jnp.nanmean(condition, axis=-1)\n</code></pre>"},{"location":"api/should/#jnlr.should.p_reduction_and_intervals","title":"p_reduction_and_intervals","text":"<pre><code>p_reduction_and_intervals(vmapped_solver, z_hat: ndarray, z_hat_samples: ndarray, alpha=0.02)\n</code></pre> <p>Estimate the probability of RMSE reduction using projected bootstrap samples, along with Clopper-Pearson confidence intervals. Args:     vmapped_solver: Function to project points onto the constraint surface.     z_hat: Forecasted points of shape <code>(batch_size, n)</code>.     z_hat_samples: Bootstrap samples of shape <code>(batch_size, num_samples, n)</code>.     alpha: Significance level for confidence intervals. Returns:     Tuple[np.ndarray, np.ndarray]: A tuple containing:         - Estimated probability of RMSE reduction for each point in <code>z_hat</code>.         - Clopper-Pearson confidence intervals for each estimate.</p> Source code in <code>src/jnlr/should.py</code> <pre><code>def p_reduction_and_intervals(vmapped_solver, z_hat: jnp.ndarray, z_hat_samples: jnp.ndarray, alpha=0.02):\n    r\"\"\"\n    Estimate the probability of RMSE reduction using projected bootstrap samples,\n    along with Clopper-Pearson confidence intervals.\n    Args:\n        vmapped_solver: Function to project points onto the constraint surface.\n        z_hat: Forecasted points of shape `(batch_size, n)`.\n        z_hat_samples: Bootstrap samples of shape `(batch_size, num_samples, n)`.\n        alpha: Significance level for confidence intervals.\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: A tuple containing:\n            - Estimated probability of RMSE reduction for each point in `z_hat`.\n            - Clopper-Pearson confidence intervals for each estimate.\n    \"\"\"\n\n    z_tilde = vmapped_solver(z_hat)\n    z_tilde_samples = vmapped_solver(z_hat_samples.reshape(-1, z_hat.shape[-1])).reshape(z_hat_samples.shape)\n    delta_pi = z_tilde - z_hat\n\n    delta_tilde_samples = z_tilde_samples - z_tilde[:, None, :]\n    norm_delta_pi = jnp.linalg.norm(delta_pi, axis=-1, keepdims=True)\n    scalar_product = jnp.sum(delta_tilde_samples * delta_pi[:, None, :], axis=-1)\n    threshold = norm_delta_pi ** 2 / 2\n    condition = scalar_product &gt; - threshold\n\n    # cutoff\n    delta_tilde_hat_samples = z_tilde_samples - z_hat[:, None, :]\n    post_rec_dist = jnp.sum(delta_tilde_hat_samples ** 2, axis=-1)\n    bootstrap_errs = z_hat_samples - z_hat[:, None, :]\n    pre_rec_dist = jnp.sum(bootstrap_errs ** 2, axis=-1)\n    max_pre_rec_dist = jnp.max(pre_rec_dist, axis=-1, keepdims=True)\n    condition = jnp.where(post_rec_dist &lt; max_pre_rec_dist, condition, jnp.nan)\n\n    # compute clopper-pearson intervals\n    intervals = jax.vmap(clopper_pearson_intervals, in_axes=(0, None, None))(jnp.nansum(condition, axis=-1),\n                                                                             condition.shape[-1], alpha)\n    return jnp.nanmean(condition, axis=-1), intervals, delta_pi\n</code></pre>"},{"location":"api/stats/","title":"Stats","text":""},{"location":"api/stats/#jnlr.stats","title":"jnlr.stats","text":""},{"location":"api/stats/#jnlr.stats.clopper_pearson_intervals","title":"clopper_pearson_intervals","text":"<pre><code>clopper_pearson_intervals(k: int, n: int, alpha: float = 0.05)\n</code></pre> <p>Compute Clopper-Pearson confidence intervals for a binomial proportion. Args:     k: number of successes     n: number of trials     alpha: significance level (default 0.05 for 95% CI)</p> <p>Returns:</p> Type Description <code>(lower_bound, upper_bound)</code> <p>tuple of lower and upper bounds of the confidence interval</p> Source code in <code>src/jnlr/stats.py</code> <pre><code>@jax.jit\ndef clopper_pearson_intervals(k:int, n:int, alpha:float=0.05):\n    \"\"\"\n    Compute Clopper-Pearson confidence intervals for a binomial proportion.\n    Args:\n        k: number of successes\n        n: number of trials\n        alpha: significance level (default 0.05 for 95% CI)\n\n    Returns:\n        (lower_bound, upper_bound): tuple of lower and upper bounds of the confidence interval\n\n    \"\"\"\n    k = jnp.asarray(k, dtype=jnp.float32)\n    n = jnp.asarray(n, dtype=jnp.float32)\n\n    def case_k0(_):\n        upper = beta_ppf_approx(1 - alpha / 2, 1.0, n - k + 1)\n        return 0.0, upper\n\n    def case_kn(_):\n        lower = beta_ppf_approx(alpha / 2, k + 1, 1.0)\n        return lower, 1.0\n\n    def case_else(_):\n        lower = beta_ppf_approx(alpha / 2, k, n - k + 1)\n        upper = beta_ppf_approx(1 - alpha / 2, k + 1, n - k)\n        return lower, upper\n\n    return lax.cond(\n        k == 0,\n        case_k0,\n        lambda _: lax.cond(k == n, case_kn, case_else, None),\n        operand=None\n    )\n</code></pre>"},{"location":"examples/compute_geodesics/","title":"Geodesics","text":"<p>For a manifold \\(\\mathcal{M}\\) a Riemmanian metric \\(\\rho=\\left(\\rho_x\\right)_{x \\in \\mathcal{M}}\\) is a smooth collection of inner products \\(\\rho_x: T_x \\mathcal{M} \\times T_x \\mathcal{M} \\rightarrow \\mathbb{R}\\) on the tangent space. Usually we can think about it as a collecion of matrices given by \\(J_p^TJ_p\\) where \\(p\\in\\mathcal{M}\\). This induces a metric that can be used to weight paths on the manifold. For a curve \\(\\gamma:[a, b] \\rightarrow \\mathcal{M}\\) we define its length as:</p> \\[l(\\gamma(a, b)) = \\int_a^b\\left\\|\\gamma^{\\prime}(t)\\right\\|_\\rho d t\\] <p>A geodesic \\(\\gamma(t)\\) on a manifold \\(M\\) with metric \\(g\\) is defined as a curve with zero tangential acceleration:</p> \\[\\nabla_{\\dot{\\gamma}} \\dot{\\gamma}(t)=0\\] <p>A second more loose definition sees the geodesic as the shortest path between two points on a curved surface or manifold, generalizing the concept of a straight line in Euclidean space. That is, geodesics are paths minimizing \\(l(\\gamma)\\), fixed starting and end points \\(a\\), \\(b\\):</p> \\[\\gamma_g(a, b) = \\text{arg}\\min_{\\begin{align}\\gamma, s.t.\\\\ \\gamma(0) = a, \\\\\\gamma(1)=b\\end{align}} l(\\gamma)\\] <p>The geodesic distance is then defined similarly as:</p> \\[d_g(a, b) = \\min_{\\begin{align}\\gamma, s.t.\\\\ \\gamma(0) = a, \\\\\\gamma(1)=b\\end{align}} l(\\gamma)\\]"},{"location":"examples/compute_geodesics/#computation-of-geodesics","title":"Computation of geodesics","text":"<p>Computing geodesics is a central problem in differential geometry, geometric processing, and machine learning on manifolds. Several approaches exist, each with trade-offs:</p> <ul> <li>MMP (Mitchell\u2013Mount\u2013Papadimitriou) algorithm   Computes exact geodesics on triangulated meshes. The only approximation is due to mesh discretization.</li> <li>PRO: highly accurate</li> <li> <p>CONS: restricted to triangulated 2D manifolds in \\(\\mathbb{R}^3\\)</p> </li> <li> <p>Graph-based methods   Given samples from \\(\\mathcal{M}\\), build a local connectivity graph (e.g. via KD-trees) and approximate geodesics by shortest paths in the graph.</p> </li> <li>PRO: works in arbitrary dimensions; efficient</li> <li> <p>CONS: accuracy depends heavily on sampling density and neighborhood construction</p> </li> <li> <p>Shooting methods (geodesic ODE integration)   Solve the geodesic equation \\(\\nabla_{\\dot{\\gamma}} \\dot{\\gamma} = 0\\) as an initial value problem: \"shoot\" from \\(a\\) with an initial velocity and adjust to reach \\(b\\).</p> </li> <li>PRO: general-purpose, works in arbitrary dimensions</li> <li> <p>CONS: computationally expensive, sensitive to initialization</p> </li> <li> <p>Heat method (Crane et al., 2013)   Approximate geodesic distance by solving a short-time heat diffusion problem, followed by normalization of the gradient field. Geodesics can then be traced along the induced distance field.</p> </li> <li>PRO: robust, scalable, works on meshes and point clouds</li> <li>CONS: gives approximate distances; requires solving PDEs numerically</li> </ul>"},{"location":"examples/compute_geodesics/#mmp","title":"MMP","text":"<p>In thefollowing we instantiate the <code>GeodesicSolver</code> class from <code>jnlr.geodesics.compute</code>. The method needs a triangulation of the function. We can pass a mesh specifying the <code>mesh</code> parameter (we can pre-compute one using the methods in <code>jnlr.utils.meses</code>) or by specifying the 2D range via tuples, and the number of points of the mesh. In this case the mesh is automatically computed by <code>GeodesicSolver</code>:</p> <pre><code>from jnlr.geodesics.compute import GeodesicSolver\nfrom jnlr.utils.manifolds import f_ackley as f_expl\nimport jax.numpy as jnp\nfrom jnlr.utils.plot_utils import plot_mesh_plotly\n\n\nranges = ((-1.3, 1.3), (-1.3, 1.3))\ngs = GeodesicSolver(f_expl, n_samples=500, ranges=ranges)\n</code></pre> <pre><code>\u001b[94m2026-01-14 15:41:17,072\u001b[96m INFO jnlr.geodesics.compute\u001b[95m: Mesh not provided. I'm building and storing a default mesh on the range ((-1.3, 1.3), (-1.3, 1.3)).\n</code></pre> <p>We can now compute the geodesic between two arbitrary points inside the range, and plot it using <code>plot_mesh_plotly</code></p> <pre><code>x0 = jnp.array([-1, -1])\nx1 = jnp.array([1, 1])\nz0 = jnp.hstack([x0, f_expl(x0)])\nz1 = jnp.hstack([x1, f_expl(x1)])\npath, gdist = gs.geodesic(z0, z1)\nplot_mesh_plotly(**gs.mesh, title=\"Geodesic on Ackley function\", lines=path, points=[z0, z1], colorscale=\"Purples\")\n</code></pre> <p></p>"},{"location":"examples/compute_geodesics/#graph-geodesic","title":"Graph geodesic","text":"<p>A faster method is the graph-based geodesics. When using a vanilla sampling, as done internally by <code>GeodesicSolver</code> when samples are not specified, it's not very accurate</p> <pre><code>gs_graph = GeodesicSolver(f_expl, n_samples=5000, ranges=ranges, method='graph')\npath_graph, gdist_graph = gs_graph.geodesic(z0, z1)\nplot_mesh_plotly(**gs.mesh, title=\"Geodesic on Ackley function\", lines=path + path_graph, show_edges=True, points=[z0, z1])\n</code></pre> <pre><code>\u001b[94m2026-01-14 15:41:17,569\u001b[96m INFO jnlr.geodesics.compute\u001b[95m: Samples not provided. I'm generating and storing default samples in the 0-1 hypercube.\n</code></pre> <p></p> <p>A better result can be obtained by providing a non-trivial sampling of the manifold, e.g. a volume-based sampling via the <code>sample</code> function inside <code>jnlr.utils.samplers</code>. This sampling is a bit slower, but more representative of the manifold surface (and must be computed just once, as it's then stored in the class instance).</p> <pre><code>from jnlr.utils.samplers import sample\nfrom jnlr.utils.plot_utils import plot_3d_projection\nsamples = sample(phi=f_expl, method='volume', n_samples=5000, bounds=ranges)\n\ngs_graph_vv = GeodesicSolver(f_expl, samples=samples, ranges=ranges, method='graph')\npath_graph_vv, gdist_graph_vv = gs_graph_vv.geodesic(z0, z1)\n</code></pre> <pre><code>plot_3d_projection(samples, f_expl)\n</code></pre> <p></p> <pre><code>path_graph_vv, gdist = gs_graph_vv.geodesic(z0, z1)\nplot_mesh_plotly(**gs.mesh, title=\"Geodesic on Ackley function\", lines=path + path_graph + path_graph_vv, show_edges=True, points=[z0, z1])\n</code></pre> <p></p> <pre><code>print('geodesic distance \\n exact {:0.2e}\\n graph {:0.2e}\\n graph volume sampled {:0.2e}\\n'.format(gdist, gdist_graph, gdist_graph_vv))\n</code></pre> <pre><code>geodesic distance \n exact 5.70e+00\n graph 6.33e+00\n graph volume sampled 5.70e+00\n</code></pre>"},{"location":"examples/compute_geodesics/#geodesic-based-scores","title":"Geodesic based scores","text":"<p>The <code>GeodesicSolver</code> class can be used to compute probabilistic scores on the manifold. One method is the <code>pointcloud_distance</code> . This method computes:</p> \\[ D_g = \\frac{1}{N}\\sum_{i=1}^N d_g(\\tilde{z}_i, z) \\] <p>where \\(\\tilde{z}_i \\in \\mathcal{M}\\) are atoms of a predictive distribution, \\(z \\in \\mathcal{M}\\) is the ground truth realization and \\(d_g\\) is the geodesic distance on \\(\\in \\mathcal{M}\\)</p> <pre><code>import jax\nimport numpy as np\n\n# define an example of prediction starting from a ground truth z0 and perturbing its independent coordinates with a gaussian\nn_samples = 100\nperturbations = 0.3*jax.random.normal(jax.random.PRNGKey(0), (n_samples, 2)) + jnp.array([1.3, 1.3])\nz_tilde = jnp.vstack([jnp.hstack([x0+pert, f_expl(x0+pert)]) for pert in perturbations])\n\n# compute the point cloud geodesic distance\nD_g = gs_graph_vv.pointcloud_distance(z_tilde, z0)\nprint('Pointcloud distance D_g {:0.3e}'.format(D_g[0]))\n</code></pre> <pre><code>Pointcloud distance D_g 3.960e+00\n</code></pre> <p>We can retrieve all the geodesic paths by using the <code>geodesic</code> method as before, but this time repeating the destination point \\(z\\) for all the \\(\\tilde{z}_i\\):</p> <pre><code>gamma_gs, _ = gs_graph_vv.geodesic(z_tilde, jnp.tile(z0, len(z_tilde)).reshape(-1, 3))\n</code></pre> <pre><code>plot_mesh_plotly(**gs.mesh, title=\"Geodesic on Ackley function\", lines=gamma_gs, show_edges=True, points=[z0] + [z_i for z_i in z_tilde], line_color='red')\n</code></pre> <p></p>"},{"location":"examples/meshes/","title":"Generate meshes","text":"<p>In the following, we show how to generate meshes for various manifolds using the utilities in <code>jnlr.utils.meshes</code>.</p>"},{"location":"examples/meshes/#3d-explicit-parameterization","title":"3D Explicit parameterization","text":"<p>The following examples take in a parameterization function and generate a mesh by evaluating the function on a grid. The functions must be in the form <code>f(U)</code> where <code>U</code> is a tuple of parameters <code>(u, v)</code> and the output is either a scalar or a 3D point <code>(x, y, z)</code>.</p>"},{"location":"examples/meshes/#3d-explicit-scalar-output-standard-parametrization","title":"3D Explicit, scalar output, standard parametrization","text":"<p>The following example generates a mesh for the Shubert function, which is a standard test function for optimization. The parametrization is the standard euclidean one, i.e., the output is a scalar value <code>z = f(x, y)</code>.</p> <pre><code>from jnlr.utils.plot_utils import plot_mesh_plotly\nfrom jnlr.utils.meshes import get_mesh\nfrom jnlr.utils.manifolds import f_shubert\n\nV, F = get_mesh(f_shubert, 'explicit', nu=50, nv=50, grid_ranges=((-1, 1), (-1, 1)))\nplot_mesh_plotly(V, F, title=\"Shubert function\")\n</code></pre> <p></p>"},{"location":"examples/meshes/#3d-explicit-angular-parametrization","title":"3D Explicit, angular parametrization","text":"<p>The following example generates a mesh for a torus, which is a standard example of a manifold with angular parameters. The parametrization is given by the function <code>phi_torus(U)</code> below, which maps the angular parameters <code>u</code> and <code>v</code> to 3D coordinates <code>(x, y, z)</code>.</p> <pre><code>import numpy as np\nimport jax.numpy as jnp\nR, r = 2.0, 0.7\n\ndef phi_torus(U):\n    u, v = U\n    cu, su = jnp.cos(u), jnp.sin(u)\n    cv, sv = jnp.cos(v), jnp.sin(v)\n    x = (R + r*cv) * cu\n    y = (R + r*cv) * su\n    z = r * sv\n    return jnp.stack([x, y, z])\n\nV, F = get_mesh(phi_torus, 'explicit', nu=50, nv=50, grid_ranges=((0, 2*np.pi), (0, 2*np.pi)))\nplot_mesh_plotly(V, F, title=\"Torus function\")\n</code></pre> <p></p>"},{"location":"examples/projection_hypersurfaces/","title":"Basic projection examples","text":"<p>The following cell defines a hypersurface starting from a graph (f_paraboloid) and then defines an implicit function (f_implicit_paraboloid) that returns how far a point is from the surface. The solver is then used to project points onto the surface defined by the implicit function. The graph is defined as</p> \\[z = f(x,y): \\mathbb{R}^2\\rightarrow \\mathbb{R}= x^2 + y^2\\] <p>and the implicit function is defined as</p> \\[f_{implicit}(x,y,z): \\mathbb{R}^3\\rightarrow \\mathbb{R} = f(x,y) - z = x^2 + y^2 - z\\] <p>When f_implicit(x, y, z) = 0, the point (x, y, z) lies on the surface of the paraboloid. The solver uses the f_implicit formulation to project points onto the surface.</p> <pre><code># Set JAX config before importing JAX\nNGRID=50\nimport os\nos.environ['JAX_PLATFORMS'] = \"cpu\"  # use \"cuda\" if you have a GPU\nos.environ['JAX_ENABLE_X64'] = \"1\"\n\n# generate gaussian samples in 3d and reproject them onto the paraboloid equations\nimport numpy as np\nimport jax\nfrom jnlr.reconcile import make_solver_alm_optax as make_solver\nimport jax.numpy as jnp\n\n# generate gaussian samples in 3d and reproject them onto the paraboloid equations\nn_samples = 100\nX = np.random.random((n_samples, 3))*2-1\n\n# define an implicit function. Each component of the function (in this case m=1) returns how far a point is from the surface\ndef f_paraboloid(v):\n    x, y = v\n    return x**2 + y**2\n\ndef f_implicit_paraboloid(v):\n    z = v[2]\n    return f_paraboloid(v[:2]) - z\n\nsolver = make_solver(f_implicit_paraboloid, n_iterations=30)\nX_proj = solver(X)\n\nprint(\"mean abs f before projection: {:0.2e}\".format(jnp.mean(jnp.abs(jax.vmap(f_implicit_paraboloid)(X)))))\nprint(\"mean abs f after projection: {:0.2e}\".format(jnp.mean(jnp.abs(jax.vmap(f_implicit_paraboloid)(X_proj)))))\n</code></pre> <pre><code>mean abs f before projection: 8.95e-01\nmean abs f after projection: 8.68e-10\n</code></pre> <p>In <code>test.manifolds</code> there are other examples of implicit functions that can be used to define hypersurfaces. Additionally, the <code>jnlr.function_utils</code> module provides a utility function <code>f_impl</code> that can convert a graph function into an implicit function. This is particularly useful for standard benchmark functions like Ackley and Rastrigin.</p> <pre><code>import jnlr.utils.manifolds as mfs\nfrom jnlr.utils.function_utils import f_impl\n\n\nsolver = make_solver(f_impl(mfs.f_paraboloid), n_iterations=10)\nX_proj = solver(X)\n\nprint(\"Paraboloid, mean abs f before projection: {:0.2e}\".format(jnp.mean(jnp.abs(jax.vmap(f_impl(mfs.f_paraboloid))(X)))))\nprint(\"Paraboloid, mean abs f after projection: {:0.2e}\".format(jnp.mean(jnp.abs(jax.vmap(f_impl(mfs.f_paraboloid))(X_proj)))))\n\n\nsolver = make_solver(f_impl(mfs.f_rastrigin), n_iterations=10)\nX_proj = solver(X)\n\nprint(\"Ackley, mean abs f before projection: {:0.2e}\".format(jnp.mean(jnp.abs(jax.vmap(f_impl(mfs.f_rastrigin))(X)))))\nprint(\"Ackley, mean abs f after projection: {:0.2e}\".format(jnp.mean(jnp.abs(jax.vmap(f_impl(mfs.f_rastrigin))(X_proj)))))\n</code></pre> <pre><code>Paraboloid, mean abs f before projection: 8.95e-01\nParaboloid, mean abs f after projection: 8.68e-10\nAckley, mean abs f before projection: 1.98e+01\nAckley, mean abs f after projection: 3.06e-10\n</code></pre> <pre><code>from jnlr.utils.plot_utils import plot_3d_projection\nplot_3d_projection(X, f_paraboloid, round_cutoff=None, solver_builder=make_solver, plot_history=True, n_iterations=4, n_grid=NGRID)\n</code></pre> <p></p> <pre><code>plot_3d_projection(X, mfs.f_abs, round_cutoff=None, solver_builder=make_solver, plot_history=True, n_iterations=40, n_grid=NGRID)\n</code></pre> <p></p> <pre><code>plot_3d_projection(X, mfs.f_ackley, round_cutoff=None, solver_builder=make_solver, plot_history=True, n_iterations=20, n_grid=NGRID)\n</code></pre> <p></p> <pre><code>import jax.numpy as jnp\n\ndef f_shubert(v):\n    \"\"\"\n    Shubert function. Input: array of shape (2,)\n    Use vmap externally for batching.\n    \"\"\"\n    x1, x2 = v\n    total1 = 0.0\n    total2 = 0.0\n    for j in range(1, 6):\n        total1 += j * jnp.cos((j + 1) * x1 + j)\n        total2 += j * jnp.cos((j + 1) * x2 + j)\n    return total1 * total2 / 100\n\nplot_3d_projection(X, f_shubert, round_cutoff=None, solver_builder=make_solver, plot_history=True, n_iterations=20, n_grid=NGRID)\n</code></pre> <p></p> <pre><code>plot_3d_projection(X, lambda z: mfs.f_rastrigin(z)/100, round_cutoff=None, solver_builder=make_solver, plot_history=True, n_iterations=20, n_grid=NGRID)\n</code></pre> <p></p> <pre><code># generate gaussian samples in 3d and reproject them onto the paraboloid equations\nn_samples = 100\nX = np.random.random((n_samples, 3))*2-1\n\n# define an implicit function for the sphere. Each component of the function (in this case m=1) returns how far a point is from the surface\n\ndef f_implicit_sphere(v):\n    return v[0]**2 + v[1]**2 + v[2]**2 -1\n\nplot_3d_projection(X, f_implicit=f_implicit_sphere, round_cutoff=None, solver_builder=make_solver, plot_history=True, n_iterations=20, n_grid=NGRID)\n</code></pre> <p></p>"},{"location":"examples/samplers/","title":"Samplers","text":"<p>The samplers module provides various sampling algorithms to generate samples from a given probability distribution. These algorithms can be used to sample a explicit (graph) or implicit (\\(f(z)=0\\)) distribution. The samplers can be used for various tasks such as: * build a dataset or mesh for a manifold * visualize a distribution * build a dataset of geodesics using one of the methods in <code>geodesics.generate</code></p>"},{"location":"examples/samplers/#sample-from-an-explicit-manifold","title":"Sample from an explicit manifold","text":"<p>The following code samples from an explicit manifold defined by a potential function \\(\\phi(x):\\mathbb{R}^i\\rightarrow \\mathbb{R}^m\\), where \\(i\\) is the number of (independent) inputs and \\(m\\) the number of constraints. The dimensionality of ambient space is \\(n = i+m\\). * Using <code>randinput_expl</code> we can obtain a random sample of the manifold by sampling uniformly via latin-hypercube in the input space and projecting the samples onto the manifold using \\(\\phi\\). This method can be used to obtain a uniform sample of the input space, but it does not guarantee a uniform sample of the manifold itself, but it's fast. * Using <code>volume_expl</code> we can obtain a non-repeated uniform sample of the manifold. The function perform an over-sampling of \\(phi\\), which gets then re-sampled \\(w_i \\sim \\sqrt{\\text{det} [G(x_i)]}\\) where \\(G = J^T J\\) is the metric tensor induced by \\(\\phi\\).</p> <pre><code>import os\nos.environ['JAX_PLATFORMS'] = \"cpu\"  # use \"cuda\" if you have a GPU\nos.environ['JAX_ENABLE_X64'] = \"1\"\n\nfrom jnlr.utils.samplers import volume_expl, randinput_expl\nfrom jnlr.utils.manifolds import f_ackley as phi\nfrom jnlr.utils.plot_utils import plot_3d_projection\n\n\nY_vol = volume_expl(phi, n_samples=500, oversample=5, roi_R=5)\nfig = plot_3d_projection(Y_vol, phi)\n\n# set view from the top\nfig.update_layout(scene_camera=dict(eye=dict(x=0., y=0., z=2.5)))\n</code></pre> <p></p> <pre><code>Y_vol = randinput_expl(phi, n_samples=500)\nfig = plot_3d_projection(Y_vol, phi)\n# set view from the top\nfig.update_layout(scene_camera=dict(eye=dict(x=0., y=0., z=2.5)))\n</code></pre> <p></p> <pre><code>from jnlr.utils.samplers import langevin_implicit\n\ndef f(xyz):\n    x, y, z = xyz\n    return x**4 - x**2 + y**2 + z**2 - 2*x*z\nY_lang = langevin_implicit(f, n_samples=3000, burn=100, thin=2, sigma=0.1, lam=0.01, kappa=0.02, R=3.0)\nfig = plot_3d_projection(Y_lang)\nfig.update_layout(scene_camera=dict(eye=dict(x=0., y=0., z=2.5)))\n</code></pre> <p></p> <pre><code>from jnlr.utils.implicit_hypersurfaces import f_dodecahedron\nY_lang = langevin_implicit(f_dodecahedron, n_samples=3000, burn=100, thin=2, sigma=0.1, lam=0.01, kappa=0.02, R=3.0)\nfig = plot_3d_projection(Y_lang)\nfig.update_layout(scene_camera=dict(eye=dict(x=0., y=0., z=2.5)))\n</code></pre> <p></p> <pre><code>from jnlr.utils.plot_utils import plot_3d_projection\nfrom jnlr.utils.implicit_hypersurfaces import surface7, surface_b, surface_c\n\nY_lang = langevin_implicit(surface7, n_samples=5000, burn=100, thin=1, sigma=0.3, lam=0, kappa=0, R=5.0, tol=1e-2)\nfig = plot_3d_projection(Y_lang)\nfig.update_layout(scene_camera=dict(eye=dict(x=0., y=0., z=2.5)))\n</code></pre> <p></p> <pre><code>Y_lang = langevin_implicit(surface_c, n_samples=5000, burn=100, thin=1, sigma=0.5, lam=0.1, kappa=0, R=5.0, tol=1e-3)\nfig = plot_3d_projection(Y_lang)\nfig.update_layout(scene_camera=dict(eye=dict(x=0., y=0., z=2.5)))\n</code></pre> <p></p> <pre><code>Y_lang = langevin_implicit(surface_b, n_samples=5000, burn=100, thin=1, sigma=0.3, lam=0, kappa=0, R=5.0, tol=1e-2)\nfig = plot_3d_projection(Y_lang)\nfig.update_layout(scene_camera=dict(eye=dict(x=0., y=0., z=2.5)))\n</code></pre> <p></p>"}]}